# cuda里面有哪些LSTM算子，以及网络如何映射到算子中间去的

**算子**：首先什么是算子，算子是一个操作，作用于一个输入得到一个输出。深度学习中常见的算子包括，矩阵乘法，非线性算子，卷积算子等。一般而言，认为是预定义的高等函数。





cuda使用的cuDNNLSTM库或者cuDNN。

参考该文献：[滴滴云 LSTM 优化之路 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/44524284)



接下来为了了解该部分，继续了解lstm的步骤：
首先基本的lstm是链式的，当前时间点的神经元接受上一个时间点的神经元的结果。

![image-20240910164028320](../../Typora_Picture/image-20240910164028320.png)



第一个门是遗忘门：
![image-20240910164149080](../../Typora_Picture/image-20240910164149080.png)

f是遗忘门的值用以表示是否完全遗忘。

第二个门是记忆门：
![image-20240910164712080](../../Typora_Picture/image-20240910164712080.png)

其中i是输入门的值，C是候选值。据遗忘门和输入门的决定来更新记忆细胞。将细胞状态与遗忘门的值相乘，表示我们遗忘了一部分的状态信息；然后将输入门的值与候选值相乘并加上去，表示我们添加了一部分新的状态信息。

![image-20240910170603689](../../Typora_Picture/image-20240910170603689.png)

第三个门是输出门：
![image-20240910164951979](../../Typora_Picture/image-20240910164951979.png)







接下来使用tensorflow的库调用CUDNNLSTM模型。



然而，在tensorflow版本2.0之后，你不需要指定CuDNNLSTM。你可以只使用没有激活功能的LSTM，它会自动使用CuDNN版本。你必须先安装CuDNN。

这部分参考[LSTM的cuda加速 | ZHIPENG个人笔记 (mintlucas.github.io)](https://mintlucas.github.io/2019/09/18/LSTM的cuda加速/)



gitclone到了源码，目标源码是lstm_ops_gpu.cu.cc







前提：
cuda将线程抽象为grid，block，thread三个层次。

一个cuda设置就是一个grid，grid的最小单位是block。grid可以是一个一维、二维、三维矩阵。

相关的重要数据有：girdDim.x 代表x轴上的block的数量

gridDim.y代表y轴上的block的数量。

同样的blockDim也是有x方向的thread的数量，y方向的thread的数量。



根据源码，发现在block中，以一种2D的形式，通过线程的(x,y)与threadIdx确定了一个处理批次的编号以及对应的cell的维度。以此确定处理的部分。这样，每一个批次数据的输入与cell的某个维度的计算交付给某个线程。

对于batch_size个的数据，门的cell_size的维度来说，线程每次计算完毕都会产生4\*batch_size\*cell_size个门数据以及batch_size，因此需要gate进行存储。偏移量b则是每个门都有一个的，因此按照维度划分的话，就是4*cell_size个。

接下来就是关于线程的部分，首先是一个线程块中的一个线程对应一个批次的特征。

主要函数是宏DECLARE_GPU_FBPROP，里面调用了前向传播函数LSTMBlockCellFprop与反向传播函数LSTMBlockCellBprop。这里以前向传播函数作为例子，
